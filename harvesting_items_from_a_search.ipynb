{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest items from a search in RecordSearch\n",
    "\n",
    "Ever searched for items in RecordSearch and wanted to save the results as a CSV file, or in some other machine-readable format? This notebook walks you through the process of creating, managing, and saving item searches – all the way from search terms to downloadable dataset. You can even download all the images from items that have been digitised!\n",
    "\n",
    "RecordSearch doesn't currently have an option for downloading machine-readable data. So to get collection metadata in a structured form, we have to resort of screen-scraping. This notebook uses the [RecordSearch Data Scraper](https://wragge.github.io/recordsearch_data_scraper/) to do most of the work.\n",
    "\n",
    "Note that the RecordSearch Data Scraper caches results to improve efficiency. This also makes it easy to resume a failed harvest. If you want to completely refresh a harvest, then delete the `cache_db.sqlite` file to start from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Import what we need](#1.-Import-what-we-need)\n",
    "2. [Available search parameters](#2.-Available-search-parameters)\n",
    "3. [Create a search](#3.-Create-a-search)\n",
    "4. [Changing how your search results are delivered](#4.-Changing-how-your-search-results-are-delivered)\n",
    "5. [Harvesting a complete set of search results](#5.-Harvesting-a-complete-set-of-search-results)\n",
    "6. [Saving the harvested results](#6.-Save-the-harvested-results)\n",
    "7. [Saving images from digitised files](#7.-Saving-images-from-digitised-files)\n",
    "8. [Saving digitised files as PDFs](#8.-Saving-digitised-files-as-PDFs)\n",
    "\n",
    "If you know the search parameters you need you can skip straight down to [start a harvest](#Start-a-harvest).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "from IPython.display import display, FileLink, HTML\n",
    "import requests\n",
    "from slugify import slugify\n",
    "from recordsearch_data_scraper.scrapers import *\n",
    "\n",
    "# This is a workaround for a problem with tqdm adding space to cells\n",
    "HTML(\"\"\"\n",
    "    <style>\n",
    "    .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
    "      padding: 0;\n",
    "      border: 0;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Available search parameters\n",
    "\n",
    "The available search parameters are the same as those in RecordSearch's Advanced Search form. There's lots of them, but you'll probably only end up using a few like `kw` and `series`. Note that you can use \\* for wildcard searches as you can in the web interface. So setting `kw` to 'wragge\\*' will find both 'wragge' and 'wragges'.\n",
    "\n",
    "See the [RecordSearch Data Scraper documentation](https://wragge.github.io/recordsearch_data_scraper/scrapers.html#RSItemSearch) for more information on search parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `kw` – string containing keywords to search for\n",
    "* `kw_options` – how to interpret `kw`, possible values are:\n",
    "    * 'ALL' – return results containing all of the keywords (default)\n",
    "    * 'ANY' – return results containg any of the keywords\n",
    "    * 'EXACT' – treat `kw` as a phrase rather than a list of words\n",
    "* `kw_exclude` – string containing keywords to exclude from search\n",
    "* `kw_exclude_options` – how to interpret `kw_exclude`, possible values are:\n",
    "    * 'ALL' – exclude results containing all of the keywords (default)\n",
    "    * 'ANY' – exclude results containg any of the keywords\n",
    "    * 'EXACT' – treat `kw_exact` as a phrase rather than a list of words\n",
    "* `search_notes` – set to 'on' to search item notes as well as metadata\n",
    "* `series` – search for items in this series\n",
    "* `series_exclude` – exclude items from this series\n",
    "* `control` – search for items matching this control symbol\n",
    "* `control_exclude` – exclude items matching this control symbol\n",
    "* `item_id` – search for items with this item ID number (formerly called `barcode`)\n",
    "* `date_from` – search for items with a date (year) greater than or equal to this, eg. '1935'\n",
    "* `date_to` – search for items with a date (year) less than or equal to this\n",
    "* `formats` – limit search to items in a particular format, see possible values below\n",
    "* `formats_exclude` – exclude items in a particular format, see possible values below\n",
    "* `locations` – limit search to items held in a particular location, see possible values below\n",
    "* `locations_exclude` – exclude items held in a particular location, see possible values below\n",
    "* `access` – limit to items with a particular access status, see possible values below\n",
    "* `access_exclude` – exclude items with a particular access status, see possible values below\n",
    "* `digital` – set to `True` to limit to items that are digitised\n",
    "\n",
    "\n",
    "Possible values for `formats` and `formats_exclude`: \n",
    "\n",
    "* 'Paper files and documents'\n",
    "* 'Index cards'\n",
    "* 'Bound volumes'\n",
    "* 'Cartographic records'\n",
    "* 'Photographs'\n",
    "* 'Microforms'\n",
    "* 'Audio-visual records'\n",
    "* 'Audio records'\n",
    "* 'Electronic records'\n",
    "* '3-dimensional records'\n",
    "* 'Scientific specimens'\n",
    "* 'Textiles'\n",
    "\n",
    "Possible values for `locations` and `locations_exclude`:\n",
    "\n",
    "* 'NAT, ACT'\n",
    "* 'Adelaide'\n",
    "* 'Australian War Memorial'\n",
    "* 'Brisbane'\n",
    "* 'Darwin'\n",
    "* 'Hobart'\n",
    "* 'Melbourne'\n",
    "* 'Perth'\n",
    "* 'Sydney'\n",
    "\n",
    "Possible values for `access` and `access_exclude`:\n",
    "\n",
    "* 'OPEN'\n",
    "* 'OWE'\n",
    "* 'CLOSED'\n",
    "* 'NYE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a search\n",
    "\n",
    "Once you've decided on your parameters you can use them to create a search. For example, if we wanted to find all items that included the word 'wragge' and were digitised, our parameters would be:\n",
    "\n",
    "* `kw='wragge'`\n",
    "* `digital=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the search\n",
    "search = RSItemSearch(kw='wragge', digital=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a look to see how many results there are in the complete results set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display total results\n",
    "search.total_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first page of results, use `.get_results()`. Note that by default there are 20 search results on a page. Subsequent calls to `.get_results()` will retrieve the next page of results. You can use this to work your way through the complete results set. That's how we'll harvest all the items in a search below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'series': 'A2479',\n",
       " 'control_symbol': '17/1306',\n",
       " 'title': 'The Wragge Estate. Property for sale.',\n",
       " 'identifier': '149309',\n",
       " 'access_status': 'Open',\n",
       " 'location': 'Canberra',\n",
       " 'contents_date_str': '1917 - 1917',\n",
       " 'contents_start_date': '1917',\n",
       " 'contents_end_date': '1917',\n",
       " 'digitised_status': True}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = search.get_results()\n",
    "\n",
    "# Show the first result\n",
    "items['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Changing how your search results are delivered\n",
    "\n",
    "There are some additional parameters that affect the way the search results are delivered.\n",
    "\n",
    "* `page` – return a specific page of research results\n",
    "* `results_per_page` – default is 20\n",
    "* `sort` – return results in a specified order, possible values:\n",
    "    * 1 – series and control symbol\n",
    "    * 3 – title\n",
    "    * 5 – start date\n",
    "    * 7 – digitised items first\n",
    "    * 12 – items with pdfs first\n",
    "    * 9 – barcode\n",
    "    * 11 – audio visual items first\n",
    "* `record_detail` – controls the amount of information included in each item record:\n",
    "    * 'brief' (default) – just the info in the search results\n",
    "    * 'digitised' – add the number of pages if the file is digitised (slower)\n",
    "    * 'full' – get the full individual record for each result (slowest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the search above, but ask for the full record details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the search\n",
    "# Edit the search parameters as desired.\n",
    "results = RSItemSearch(record_detail='full', kw='wragge', digital=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the first result again, we'll see that it contains some extra fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Wragge Estate. Property for sale.',\n",
       " 'identifier': '149309',\n",
       " 'series': 'A2479',\n",
       " 'control_symbol': '17/1306',\n",
       " 'digitised_status': True,\n",
       " 'digitised_pages': 4,\n",
       " 'access_status': 'Open',\n",
       " 'access_decision_reasons': [],\n",
       " 'location': 'Canberra',\n",
       " 'retrieved': '2021-05-22T16:57:47.120328+10:00',\n",
       " 'contents_date_str': '1917 - 1917',\n",
       " 'contents_start_date': '1917',\n",
       " 'contents_end_date': '1917',\n",
       " 'access_decision_date_str': '22 Aug 1974',\n",
       " 'access_decision_date': '1974-08-22'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = results.get_results()\n",
    "\n",
    "# Show the first result\n",
    "items['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Harvesting a complete set of search results\n",
    "\n",
    "Ok, we've learnt how to create a search and get back some data, but only getting the first 20 results is not so useful. What if our search contains hundreds or thousands of items? How do we get them all?\n",
    "\n",
    "To save everything, we have to loop through each page in the result set, saving the results as we go. The cell below does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "# Initialise the search -- edit the search parameters below as desired\n",
    "search = RSItemSearch(kw='wragge', digital=True)\n",
    "with tqdm(total=search.total_results) as pbar:\n",
    "    more = True\n",
    "    while more:\n",
    "        # Get a page of results\n",
    "        data = search.get_results()\n",
    "        if data['results']:\n",
    "            # Add the page of results to the items list\n",
    "            items += data['results']\n",
    "            pbar.update(len(data['results']))\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            more = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! You might have noticed that RecordSearch only displays results for searches that return fewer than 20,000 items. Because the screen scraper is just extracting details from the RecordSearch web pages, the 20,000 limit applies here as well. If your search has more than 20,000 results, you'll need to narrow it down using additional parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of splitting a large search up into harvestable chunks is to use wildcard values and the `control` search parameter. For example, series B13 has more than 20,000 items, but if we limit the results to items with a control symbol starting with '1', we bring the number down to under 20,000:\n",
    "\n",
    "To make sure we get everything in the series we can repeat the harvest using a range of prefixes for the control symbol – the easiest approach is simply to loop through each letter and number from A to Z and 0 to 9. That's exactly what the `harvest_series()` function below does if there are more than 20,000 results in a search.\n",
    "\n",
    "Note that it's possible to get duplicate items this way because some items include earlier versions of control symbols and these are searched as well as the current ones. These can be removed from the saved harvests by using something like Pandas `.drop_duplicates()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is basically a list of letters and numbers that we can use to build up control symbol values.\n",
    "control_range = [str(number) for number in range(0, 10)] + [letter for letter in string.ascii_uppercase] + ['/']\n",
    "\n",
    "def get_results(**kwargs):\n",
    "    '''\n",
    "    Save all the results from a search using the given parameters.\n",
    "    If there are more than 20,000 results, return False.\n",
    "    Otherwise, return the harvested items.\n",
    "    '''\n",
    "    s = RSItemSearch(**kwargs)\n",
    "    if s.total_results == '20,000+':\n",
    "        return False\n",
    "    else:\n",
    "        items = []\n",
    "        with tqdm(total=s.total_results, leave=False) as pbar:\n",
    "            more = True\n",
    "            while more:\n",
    "                data = s.get_results()\n",
    "                if data['results']:\n",
    "                    items += data['results']\n",
    "                    pbar.update(len(data['results']))\n",
    "                    time.sleep(0.5)\n",
    "                else:\n",
    "                    more = False\n",
    "        return items\n",
    "    \n",
    "def refine_controls(current_control, **kwargs):\n",
    "    '''\n",
    "    Add additional letters/numbers to the control symbol wildcard search \n",
    "    until the number of results is less than 20,000.\n",
    "    Then harvest the results.\n",
    "    Returns:\n",
    "        * the RSItemSearch object (containing the search params, total results etc)\n",
    "        * a list containing the harvested items\n",
    "    '''\n",
    "    items = []\n",
    "    for control in  control_range:\n",
    "        new_control = current_control.strip('*') + control + '*'\n",
    "        # print(new_control)\n",
    "        kwargs['control'] = new_control\n",
    "        results = get_results(**kwargs)\n",
    "        # print(total)\n",
    "        if results is False:\n",
    "            items += refine_controls(new_control, **kwargs)\n",
    "        else:\n",
    "            items += results\n",
    "    return items\n",
    "\n",
    "def harvest_search(**kwargs):\n",
    "    '''\n",
    "    Harvest all the items from a search using the supplied parameters.\n",
    "    If there are more than 20,000 results, it will use control symbol \n",
    "    wildcard values to try and split the results into harvestable chunks.\n",
    "    '''\n",
    "    # Initialise the search\n",
    "    search = RSItemSearch(**kwargs)\n",
    "    # If there are more than 20,000 results, try chunking using control symbols\n",
    "    if search.total_results == '20,000+':\n",
    "        items = []\n",
    "        # Loop through the letters and numbers\n",
    "        for control in control_range:\n",
    "            # Add letter/number as a wildcard value\n",
    "            kwargs['control'] = f'{control}*'\n",
    "            # Try getting the results\n",
    "            results = get_results(**kwargs)\n",
    "            if results:\n",
    "                items += results\n",
    "            # If there's still more than 20,000, add more letters/numbers to the control symbol!\n",
    "            else:\n",
    "                items += refine_controls(control, **kwargs)\n",
    "    # If there's less than 20,000 results, save them all\n",
    "    else:\n",
    "        items = get_results(**kwargs)\n",
    "    return search, items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a harvest\n",
    "\n",
    "Insert your search parameters in the brackets below.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* `search, items = harvest_search(kw='rabbit')`\n",
    "* `search, items = harvest_search(kw='rabbit', digital=True)`\n",
    "* `search, items = harvest_search(record_detail='full', kw='rabbit', series='A1)`\n",
    "* `search, items = harvest_search(series='B13')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search, items = harvest_search(series='B13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the harvested results\n",
    "\n",
    "Once the harvest is finished we can save the results. The function below creates a directory for the harvest, and saves three files inside:\n",
    "\n",
    "* `metadata.json` – this is a summary of your harvest, including the parameters you used and the date it was run\n",
    "* `results.jsonl` – this is the harvested data with each record saved as a JSON object on a new line\n",
    "* `results.csv` – the harvested data saved as a CSV file (if you've saved 'full' records, the list of `access_decision_reasons` will be saved as a pipe-separated string)\n",
    "\n",
    "The `metadata.json` file looks something like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"date_harvested\": \"2021-05-22T22:05:10.705184\", \n",
    "    \"search_params\": {\"results_per_page\": 20, \"sort\": 9, \"record_detail\": \"digitised\"}, \n",
    "    \"search_kwargs\": {\"kw\": \"wragge\"}, \n",
    "    \"total_results\": 208, \n",
    "    \"total_harvested\": 208\n",
    "}\n",
    "```\n",
    "\n",
    "The fields in the results files are:\n",
    "\n",
    "* `title`\n",
    "* `identifier` \n",
    "* `series` \n",
    "* `control_symbol`\n",
    "* `digitised_status`\n",
    "* `digitised_pages` – if `record_detail` is set to 'digitised' or 'full'\n",
    "* `access_status`\n",
    "* `access_decision_reasons` – if `record_detail` is set to 'full'\n",
    "* `location`\n",
    "* `retrieved` – date/time when this record was retrieved from RecordSearch\n",
    "* `contents_date_str`\n",
    "* `contents_start_date`\n",
    "* `contents_end_date`\n",
    "* `access_decision_date_str` – if `record_detail` is set to 'full'\n",
    "* `access_decision_date` – if `record_detail` is set to 'full'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_harvest(search, items):\n",
    "    params = search.params.copy()\n",
    "    params.update(search.kwargs)\n",
    "    today = datetime.now()\n",
    "    search_param_str = '_'.join(sorted([f'{k}_{v}' for k, v in params.items() if v is not None and k not in ['results_per_page', 'sort']]))\n",
    "    data_dir = Path('harvests', f'{today.strftime(\"%Y%m%d\")}_{search_param_str}')\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    metadata = {\n",
    "        'date_harvested': today.isoformat(),\n",
    "        'search_params': search.params,\n",
    "        'search_kwargs': search.kwargs,\n",
    "        'total_results': search.total_results,\n",
    "        'total_harvested': len(items)\n",
    "    }\n",
    "\n",
    "    with Path(data_dir, 'metadata.json').open('w') as md_file:\n",
    "        json.dump(metadata, md_file)\n",
    "\n",
    "    with Path(data_dir, 'results.jsonl').open('w') as data_file:\n",
    "        for item in items:\n",
    "            data_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    df = pd.json_normalize(items)\n",
    "    # Flatten list\n",
    "    try:\n",
    "        df['access_decision_reasons'] = df['access_decision_reasons'].apply(lambda l: ' | '.join(l))\n",
    "    except KeyError:\n",
    "        pass\n",
    "    # Remove any duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(Path(data_dir, 'results.csv'), index=False)\n",
    "    print(f'Harvest directory: {data_dir}')\n",
    "    display(FileLink(Path(data_dir, 'metadata.json')))\n",
    "    display(FileLink(Path(data_dir, 'results.jsonl')))\n",
    "    display(FileLink(Path(data_dir, 'results.csv')))\n",
    "    return str(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function returns the path to the harvest directory\n",
    "# We can use that below to save images or pdfs\n",
    "harvest_dir = save_harvest(search, items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving images from digitised files\n",
    "\n",
    "Once you've saved all the metadata from your search, you can use it to download images from all the items that have been digitised.\n",
    "\n",
    "Note that you can only save the images if you set the `record_detail` parameter to 'digitised' or 'full' in the original harvest.\n",
    "\n",
    "The function below will look for all items that have a `digitised_pages` value in the harvest results, and then download an image for each page. The images will be saved in an `images` subdirectory, inside the original harvest directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(harvest_dir):\n",
    "    df = pd.read_csv(Path(harvest_dir, 'results.csv'))\n",
    "    with tqdm(total=df.loc[df['digitised_status'] == True].shape[0], desc='Files') as pbar:\n",
    "        for item in df.itertuples():\n",
    "            image_dir = Path(f'{harvest_dir}/images/{slugify(item.series)}-{slugify(item.control_symbol)}-{item.identifier}')\n",
    "\n",
    "            # Create the folder (and parent if necessary)\n",
    "            image_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            # Loop through the page numbers\n",
    "            for page in tqdm(range(1, int(item.digitised_pages) + 1), desc='Images', leave=False):\n",
    "\n",
    "                # Define the image filename using the barcode and page number\n",
    "                filename = Path(f'{image_dir}/{item.identifier}-{page}.jpg')\n",
    "\n",
    "                # Check to see if the image already exists (useful if rerunning a failed harvest)\n",
    "                if not filename.exists():\n",
    "                    # If it doens't already exist then download it\n",
    "                    img_url = f'https://recordsearch.naa.gov.au/NaaMedia/ShowImage.asp?B={item.identifier}&S={page}&T=P'\n",
    "                    response = requests.get(img_url)\n",
    "                    response.raise_for_status()\n",
    "                    filename.write_bytes(response.content)\n",
    "\n",
    "                    time.sleep(0.5)\n",
    "            pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply the path to the directory containing the harvested data\n",
    "# This is the value returned by the `save_harvest()` function.\n",
    "# eg: 'harvests/20210522_digital_True_kw_wragge_record_detail_full'\n",
    "save_images(harvest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving digitised files as PDFs\n",
    "\n",
    "You can also save digitised files as PDFs. The function below will save any digisted files in the results to a `pdfs` subdirectory within the harvest directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pdfs(harvest_dir):\n",
    "    df = pd.read_csv(Path(harvest_dir, 'results.csv'))\n",
    "    pdf_dir = Path(harvest_dir, 'pdfs')\n",
    "    pdf_dir.mkdir(exist_ok=True)\n",
    "    with tqdm(total=df.loc[df['digitised_status'] == True].shape[0], desc='Files') as pbar:\n",
    "        for item in df.itertuples():\n",
    "            image_dir = Path(f'{harvest_dir}/images/{slugify(item.series)}-{slugify(item.control_symbol)}-{item.identifier}')\n",
    "            pdf_file = Path(pdf_dir, f'{slugify(item.series)}-{slugify(item.control_symbol)}-{item.identifier}.pdf') \n",
    "            if not pdf_file.exists():\n",
    "                pdf_url = f'https://recordsearch.naa.gov.au/SearchNRetrieve/NAAMedia/ViewPDF.aspx?B={item.identifier}&D=D'\n",
    "                response = requests.get(pdf_url)\n",
    "                response.raise_for_status()\n",
    "                pdf_file.write_bytes(response.content)\n",
    "                time.sleep(0.5)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply the path to the directory containing the harvested data\n",
    "# This is the value returned by the `save_harvest()` function.\n",
    "# eg: 'harvests/20210522_digital_True_kw_wragge_record_detail_full'\n",
    "save_pdfs(harvest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Coming soon, notebooks to help you explore your harvested data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) as part of the [GLAM Workbench](https://glam-workbench.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
