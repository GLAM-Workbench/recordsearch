{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest items from a search in RecordSearch\n",
    "\n",
    "RecordSearch, the National Archives of Australia's online database doesn't currently have an option for downloading machine-readable data. So to get collection metadata in a structured form, we have to resort of screen-scraping. All the screen-scraping code is contained in the [recordsearch_tools](https://github.com/wragge/recordsearch_tools) package.\n",
    "\n",
    "This notebook saves data from a search for items in RecordSearch. An item in RecordSearch is usually a file, but might also be a photo, a volume, or some other sort of container. \n",
    "\n",
    "Using this notebook you can save the results of a search for items as a CSV-formatted file for easy download and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>If you haven't used one of these notebooks before, they're basically web pages in which you can write, edit, and run live code. They're meant to encourage experimentation, so don't feel nervous. Just try running a few cells and see what happens!</p>\n",
    "\n",
    "<p>\n",
    "    Some tips:\n",
    "    <ul>\n",
    "        <li>Code cells have boxes around them.</li>\n",
    "        <li>To run a code cell click on the cell and then hit <b>Shift+Enter</b>. The <b>Shift+Enter</b> combo will also move you to the next cell, so it's a quick way to work through the notebook.</li>\n",
    "        <li>While a cell is running a <b>*</b> appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number.</li>\n",
    "        <li>In most cases you'll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones.</li>\n",
    "        <li>To edit a code cell, just click on it and type stuff. Remember to run the cell once you've finished editing.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<p><b>Is this thing on?</b> If you can't edit or run any of the code cells, you might be viewing a static (read only) version of this notebook. Click here to <a href=\"\">load a <b>live</b> version</a> running on Binder.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from tinydb import TinyDB\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from recordsearch_tools.client import RSSearchClient, TooManyError\n",
    "from slugify import slugify\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from IPython.display import display, FileLink\n",
    "\n",
    "# Make sure the 'data' directory exists\n",
    "Path('data').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available search parameters\n",
    "\n",
    "The available search parameters are the same as those in RecordSearch's Advanced Search form. There's lots of them, but you'll probably only end up using a few like `kw` and `series`. Note that you can use \\* for wildcard searches as you can in the web interface. So setting `kw` to 'wragge\\*' will find both 'wragge' and 'wragges'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `kw` – string containing keywords to search for\n",
    "* `kw_options` – how to interpret `kw`, possible values are:\n",
    "    * 'ALL' – return results containing all of the keywords (default)\n",
    "    * 'ANY' – return results containg any of the keywords\n",
    "    * 'EXACT' – treat `kw` as a phrase rather than a list of words\n",
    "* `kw_exclude` – string containing keywords to exclude from search\n",
    "* `kw_exclude_options` – how to interpret `kw_exclude`, possible values are:\n",
    "    * 'ALL' – exclude results containing all of the keywords (default)\n",
    "    * 'ANY' – exclude results containg any of the keywords\n",
    "    * 'EXACT' – treat `kw_exact` as a phrase rather than a list of words\n",
    "* `search_notes` – set to 'on' to search item notes as well as metadata\n",
    "* `series` – search for items in this series\n",
    "* `series_exclude` – exclude items from this series\n",
    "* `control` – search for items matching this control symbol\n",
    "* `control_exclude` – exclude items matching this control symbol\n",
    "* `barcode` – search for items with this barcode number\n",
    "* `date_from` – search for items with a date (year) greater than or equal to this, eg. '1935'\n",
    "* `date_to` – search for items with a date (year) less than or equal to this\n",
    "* `formats` – limit search to items in a particular format, see possible values below\n",
    "* `formats_exclude` – exclude items in a particular format, see possible values below\n",
    "* `locations` – limit search to items held in a particular location, see possible values below\n",
    "* `locations_exclude` – exclude items held in a particular location, see possible values below\n",
    "* `access` – limit to items with a particular access status, see possible values below\n",
    "* `access_exclude` – exclude items with a particular access status, see possible values below\n",
    "* `digital` – set to 'on' to limit to items that are digitised\n",
    "\n",
    "\n",
    "Possible values for `formats` and `formats_exclude`: \n",
    "\n",
    "* 'Paper files and documents'\n",
    "* 'Index cards'\n",
    "* 'Bound volumes'\n",
    "* 'Cartographic records'\n",
    "* 'Photographs'\n",
    "* 'Microforms'\n",
    "* 'Audio-visual records'\n",
    "* 'Audio records'\n",
    "* 'Electronic records'\n",
    "* '3-dimensional records'\n",
    "* 'Scientific specimens'\n",
    "* 'Textiles'\n",
    "\n",
    "Possible values for `locations` and `locations_exclude`:\n",
    "\n",
    "* 'NAT, ACT'\n",
    "* 'Adelaide'\n",
    "* 'Australian War Memorial'\n",
    "* 'Brisbane'\n",
    "* 'Darwin'\n",
    "* 'Hobart'\n",
    "* 'Melbourne'\n",
    "* 'Perth'\n",
    "* 'Sydney'\n",
    "\n",
    "Possible values for `access` and `access_exclude`:\n",
    "\n",
    "* 'OPEN'\n",
    "* 'OWE'\n",
    "* 'CLOSED'\n",
    "* 'NYE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search\n",
    "\n",
    "Once you've decided on your parameters you can use them to create a search. For example, if we wanted to find all items that included the word 'wragge' and were digitised, our parameters would be:\n",
    "\n",
    "* `kw='wragge'`\n",
    "* `digital='on'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search client\n",
    "c = RSSearchClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the search parameters to the client and save the results\n",
    "results =  c.search(kw='wragge', digital='on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a look to see how many results there are in the complete results set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display total results\n",
    "results['total_results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the search client only gets one page of results (containing 20 items) at a time. You can check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many results do we actually have\n",
    "len(results['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'series': 'A2479',\n",
       " 'control_symbol': '17/1306',\n",
       " 'title': 'The Wragge Estate. Property for sale.',\n",
       " 'access_status': 'Open',\n",
       " 'location': 'Canberra',\n",
       " 'contents_dates': {'date_str': '1917 - 1917',\n",
       "  'start_date': '1917',\n",
       "  'end_date': '1917'},\n",
       " 'digitised_status': True,\n",
       " 'digitised_pages': 4,\n",
       " 'identifier': '149309'}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing how your search results are delivered\n",
    "\n",
    "There are some additional parameters that affect the way the search results are delivered. We'll use some of these to harvest the complete results set.\n",
    "\n",
    "* `page` – return a specific page of research results\n",
    "* `sort` – return results in a specified order, possible values:\n",
    "    * 1 – series and control symbol\n",
    "    * 3 – title\n",
    "    * 5 – start date\n",
    "    * 7 – digitised items first\n",
    "    * 12 – items with pdfs first\n",
    "    * 9 – barcode\n",
    "    * 11 – audio visual items first\n",
    "* `digitised` – set to `True` (default) or `False` to control whether to include the number of pages in each digitised file (if `True`, extra requests are made to get the info which slows things down a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to get the second page of results from the search above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results =  c.search(kw='wragge', digital='on', page=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first item in our result set should be different, because it's coming from the second page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'series': 'A6770',\n",
       " 'control_symbol': 'WRAGGE K C',\n",
       " 'title': 'WRAGGE KEITH CLEMENT : Service Number - B/2680 : Date of birth - 16 Jan 1922 : Place of birth - BRISBANE QLD : Place of enlistment - BRISBANE : Next of Kin - RUPERT',\n",
       " 'access_status': 'Open',\n",
       " 'location': 'Canberra',\n",
       " 'contents_dates': {'date_str': '1939 - 1948',\n",
       "  'start_date': '1939',\n",
       "  'end_date': '1948'},\n",
       " 'digitised_status': True,\n",
       " 'digitised_pages': 2,\n",
       " 'identifier': '4523493'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvesting a complete set of (less than 20,000) results\n",
    "\n",
    "Ok, we've learnt how to create a search and get back some data, but only getting the first 20 results is not so useful. What if our search contains hundreds or thousands of items? How do we get them all?\n",
    "\n",
    "To save everything, we have to loop through each page in the result set, saving the results as we go. The functions below do just that.\n",
    "\n",
    "But wait! You might have noticed that RecordSearch only displays results for searches that return fewer than 20,000 items. Because the screen scraper is just extracting details from the RecordSearch web pages, the 20,000 limit applies here as well. If your search has more than 20,000 results, you'll need to narrow it down using additional parameters.\n",
    "\n",
    "The main function below is `harvest_items()`. You just give it any of the search parameters listed above. It will loop through all the pages in the result set, saving the items to a simple JSON database using TinyDB.\n",
    "\n",
    "The database will be created in the `data` directory. It's name will include a timestamp, identifying the time at which the harvest was started. For example `db-items-1567492794.json`. There are more functions for using and managing the db files below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_results(client, **kwargs):\n",
    "    '''\n",
    "    Get the total number of results returned by a search.\n",
    "    '''\n",
    "    try:\n",
    "        # Get the first page of results, passing digitised=Flase to speed things up\n",
    "        results = client.search(digitised=False, **kwargs)\n",
    "        \n",
    "        # Get the total number of results\n",
    "        total = results['total_results']\n",
    "        \n",
    "    # Uh oh there are more than 20,000 results\n",
    "    except TooManyError:\n",
    "        print('There are more than 20,000 results.')\n",
    "        total = None\n",
    "    return total\n",
    "\n",
    "def harvest_items(start=1, db_path=None, **kwargs):\n",
    "    '''\n",
    "    Harvest items from a search and save them to a database.\n",
    "    Supply any of the search parameters listed above.\n",
    "    '''\n",
    "    # Initiate the client\n",
    "    client = RSSearchClient()\n",
    "    \n",
    "    # Get the total number of results returned by this search\n",
    "    total_results = get_total_results(client, **kwargs)\n",
    "    \n",
    "    # If the number of results is between 1 and 20,000 we can harvest!\n",
    "    if total_results:\n",
    "        \n",
    "        # Calculate the number of results pages\n",
    "        total_pages = math.ceil(int(total_results) / client.results_per_page)\n",
    "        \n",
    "        # We're creating a new db\n",
    "        if not db_path:\n",
    "            \n",
    "            # Get the current timestamp\n",
    "            timestamp = int(time.time())\n",
    "            \n",
    "            # Create a new db using the timestamp as a name\n",
    "            db = TinyDB(Path(f'data/db-items-{timestamp}.json'))\n",
    "            \n",
    "            # Save the details of this harvest to the 'meta' table of the database\n",
    "            # This keeps the query metadata with the results, and helps us to restart the harvest if necessary.\n",
    "            db.table('meta').insert({\n",
    "                'timestamp': timestamp, \n",
    "                'total_results': int(total_results), \n",
    "                'total_pages': total_pages, \n",
    "                'results_per_page': client.results_per_page,\n",
    "                'params': kwargs\n",
    "            })\n",
    "        else:\n",
    "            \n",
    "            # If we have an existing db, open it\n",
    "            db = TinyDB(db_path)\n",
    "            \n",
    "        # Loop through the range of pages\n",
    "        for page in tqdm(range(start, total_pages + 1), unit='page', desc='Pages:'):\n",
    "            \n",
    "            # Get results from each page\n",
    "            # Note that sort is set to 9 (barcode) to make sure the pages stay in the same order\n",
    "            # If we don't set the sort param we can end up getting duplicates and missing records\n",
    "            items = client.search(sort=9, page=page, **kwargs)\n",
    "            \n",
    "            # Save the results from this page to the db\n",
    "            db.table('items').insert_multiple(items['results'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a harvest!\n",
    "harvest_items(kw='wragge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing harvests\n",
    "\n",
    "If you're doing a large harvest, you might find that it fails part way through. You might also want to check on the details of a past harvest, or even reharvest a query to see if anything new has been added. Because we've saved the harvest metadata and results into a TinyDB database, it's easy to perform some basic checks and management tasks.\n",
    "\n",
    "There are three main functions defined below:\n",
    "\n",
    "* `harvest_report()` – prints basic details of a harvest\n",
    "* `harvest_restart()` – restarts a failed harvest\n",
    "* `reharvest_items()` – creates a new harvest using the query settings of an existing harvest\n",
    "\n",
    "In each case you can specify the path to an existing harvest database, something like `data/db-items-1567480717.json`. If you don't specify a database, the function will assume you want the most recent.\n",
    "\n",
    "Here's an example of the output from `harvest_report()`.\n",
    "\n",
    "```\n",
    "Harvest started: 2019-09-03 15:21:11\n",
    "Items harvested: 200 of 200\n",
    "\n",
    "{'timestamp': 1567488071,\n",
    " 'total_results': 200,\n",
    " 'total_pages': 10,\n",
    " 'results_per_page': 20,\n",
    " 'params': {'kw': 'wragge'}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_db():\n",
    "    '''\n",
    "    Get the database created by the most recent harvest.\n",
    "    '''\n",
    "    p = Path('data')\n",
    "    dbs = sorted(list(p.glob('db-items-[0-9]*.json')))\n",
    "    try:\n",
    "        latest = dbs[-1]\n",
    "    except IndexError:\n",
    "        print('No databases')\n",
    "        latest = None\n",
    "    return latest\n",
    "\n",
    "def get_db(db_path):\n",
    "    '''\n",
    "    Get a harvest database.\n",
    "    If db_path is supplied then return that db.\n",
    "    If not, then return the most recently created db.\n",
    "    '''\n",
    "    db = None\n",
    "    if not db_path:\n",
    "        db_path = get_latest_db()\n",
    "    if db_path:  \n",
    "        db = TinyDB(db_path)\n",
    "    return db\n",
    "\n",
    "def harvest_report(db_path=None):\n",
    "    '''\n",
    "    Print a report of the specified harvest.\n",
    "    If db_path is not supplied, display details from the most recently created harvest.\n",
    "    '''\n",
    "    db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        meta = db.table('meta').all()[0]\n",
    "        date = datetime.fromtimestamp(meta['timestamp']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        items_harvested = len(db.table('items').all())\n",
    "        print(f'Harvest started: {date}')\n",
    "        print(f'Items harvested: {items_harvested} of {meta[\"total_results\"]}\\n')\n",
    "        display(meta)\n",
    "    \n",
    "def harvest_restart(db_path=None):\n",
    "    '''\n",
    "    Attempt to restart the specified harvest.\n",
    "    If db_path is not supplied, restart the most recently created harvest.\n",
    "    '''\n",
    "    db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        meta = db.table('meta').all()[0]\n",
    "        items_harvested = len(db.table('items').all())\n",
    "        pages_harvested = math.ceil(items_harvested / meta['results_per_page'])\n",
    "        if pages_harvested < meta['total_pages']:\n",
    "            start = pages_harvested + 1\n",
    "            harvest_items(db_path=db_path, start=start, **meta['params'])\n",
    "        else:\n",
    "            print('Harvest complete')\n",
    "            \n",
    "def reharvest_items(db_path=None):\n",
    "    '''\n",
    "    Harvest items using the parameters of the specified db.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        meta = db.table('meta').all()[0]\n",
    "        harvest_items(**meta['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest started: 2019-09-03 16:40:15\n",
      "Items harvested: 200 of 200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timestamp': 1567492815,\n",
       " 'total_results': 200,\n",
       " 'total_pages': 10,\n",
       " 'results_per_page': 20,\n",
       " 'params': {'kw': 'wragge'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display details of the most recent harvest\n",
    "# Optionally, supply the path to an existing db, eg: harvest_report('data/db-items-1567480968.json')\n",
    "harvest_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the most recent harvest\n",
    "# Optionally, supply the path to an existing db, eg: harvest_restart('data/db-items-1567480968.json')\n",
    "harvest_restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new harvest using the parameters of the most recent harvest\n",
    "# Optionally, supply the path to an existing db, eg: reharvest_items('data/db-items-1567480968.json')\n",
    "reharvest_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a harvest\n",
    "\n",
    "Although your harvest is already saved in a TinyDB database, you might want to convert it to a simpler format for download and analysis. The functions below provide two options:\n",
    "\n",
    "* `save_harvest_as_json()` – save the harvested items as a JSON file\n",
    "* `save_harvest_as_csv()` – save the harvested items as a CSV file\n",
    "\n",
    "The columns in the CSV-formatted file are:\n",
    "\n",
    "* `identifier` – the barcode number\n",
    "* `series` – identifier of the series which contains the item\n",
    "* `control_symbol` – individual control symbol\n",
    "* `title` – title of the item\n",
    "* `contents_date_str` – the contents date string as in RecordSearch\n",
    "* `contents_start_date` – the first date in the contents date string converted to ISO format\n",
    "* `contents_end_date` – the second date in the contents date string converted to ISO format\n",
    "* `location` – where the file is held\n",
    "* `access_status` – 'Closed', 'Open', 'OWE, or 'NYE\n",
    "* `digitised_status` – True/False, has the file been digitised\n",
    "* `digitised_pages` – number of pages in the digitised file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_harvest_as_json(db_path=None):\n",
    "    '''\n",
    "    Save harvested items as a json file.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        \n",
    "        # Get harvest metadata\n",
    "        meta = db.table('meta').all()[0]\n",
    "        \n",
    "        # Get harvested items\n",
    "        items = db.table('items').all()\n",
    "        \n",
    "        # Set file name and path\n",
    "        filename = Path(f'data/items-{meta[\"timestamp\"]}.json')\n",
    "        \n",
    "        # Dump items to a JSON file\n",
    "        with open(filename, 'w') as json_file:\n",
    "            json.dump(items, json_file)\n",
    "        \n",
    "        # Display link to download\n",
    "        display(FileLink(filename))\n",
    "            \n",
    "def save_harvest_as_csv(db_path=None):\n",
    "    '''\n",
    "    Save harvested items as a CSV file.\n",
    "    If db_path is not supplied, use the most recently created harvest.\n",
    "    '''\n",
    "    db = get_db(db_path)\n",
    "    if db is not None:\n",
    "        \n",
    "        # Get harvest metadata\n",
    "        meta = db.table('meta').all()[0]\n",
    "        \n",
    "        # Get harvested items\n",
    "        items = db.table('items').all()\n",
    "        \n",
    "        # Flatten the date field using json_normalise and convert the items to a dataframe\n",
    "        df = pd.DataFrame(json_normalize(items))\n",
    "        \n",
    "        # Rename the date columns\n",
    "        df.rename(columns={'contents_dates.date_str': 'contents_date_str', 'contents_dates.start_date': 'contents_start_date', 'contents_dates.end_date': 'contents_end_date'}, inplace=True)\n",
    "        \n",
    "        # Put the columns in a nice order\n",
    "        df = df[['identifier', 'series', 'control_symbol', 'title', 'contents_date_str', 'contents_start_date', 'contents_end_date', 'location', 'access_status', 'digitised_status', 'digitised_pages']]\n",
    "        \n",
    "        # Set file name and path\n",
    "        filename = Path(f'data/items-{meta[\"timestamp\"]}.csv')\n",
    "        \n",
    "        # Save as CSV\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Display link to download\n",
    "        display(FileLink(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/items-1567492815.json' target='_blank'>data/items-1567492815.json</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/recordsearch/notebooks/data/items-1567492815.json"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the most recent harvest as a json file\n",
    "# Optionally, supply the path to an existing db, eg: save_harvest_as_json('data/db-1567480968.json')\n",
    "save_harvest_as_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/items-1567492815.csv' target='_blank'>data/items-1567492815.csv</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/recordsearch/notebooks/data/items-1567492815.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the most recent harvest as a CSV file\n",
    "# Optionally, supply the path to an existing db, eg: save_harvest_as_json('data/db-1567480968.json')\n",
    "save_harvest_as_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) as part of the GLAM Workbench"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
